{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Autophagy Package\n",
    "The additional `importlib` statement guarantees that the package is reloaded when this code-block is executed.\n",
    "This is important since loaded modules are cached which means that if we had changed the `autophagy` package, a simple import statement would not be able to give us the new behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import autophagy\n",
    "importlib.reload(autophagy)\n",
    "from autophagy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Simulation Settings\n",
    "These settings are predefined by our current simulation.\n",
    "They directly control properties of the cells.\n",
    "In this example, we focus on a limited subset of parameters which are relevant for our simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimulationSettings {\n",
      "    n_cells_cargo: 1,\n",
      "    n_cells_r11: 500,\n",
      "    cell_dampening: 1.0,\n",
      "    cell_radius_cargo: 10.0,\n",
      "    cell_radius_r11: 1.0,\n",
      "    cell_mechanics_interaction_range_cargo: 3.0,\n",
      "    cell_mechanics_interaction_range_r11: 1.0,\n",
      "    cell_mechanics_random_travel_velocity: 0.025,\n",
      "    cell_mechanics_random_update_time: 50.0,\n",
      "    cell_mechanics_potential_strength: 0.5,\n",
      "    cell_mechanics_relative_clustering_strength: 0.03,\n",
      "    dt: 0.25,\n",
      "    n_times: 20001,\n",
      "    save_interval: 50,\n",
      "    n_threads: 6,\n",
      "    domain_size: 50.0,\n",
      "    storage_name: \"out/autophagy\",\n",
      "    show_progressbar: true,\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "simulation_settings = SimulationSettings()\n",
    "simulation_settings.n_times=20_001\n",
    "simulation_settings.save_interval=50\n",
    "simulation_settings.domain_size=50\n",
    "simulation_settings.cell_mechanics_random_travel_velocity=0.025\n",
    "simulation_settings.cell_mechanics_potential_strength=0.5\n",
    "simulation_settings.n_threads=6\n",
    "print(simulation_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually run the Simulation\n",
    "Running the simulation is very simple. We have previously defined the settings and will now simply call the run_simulation function.\n",
    "\n",
    "### Note!\n",
    "There are many interesting details about how `cellular_raza` works and what you can do with it. For now, these details remain hidden behind this function. If you want a fully flexible simulation framework, you are encouraged to read [the book](https://jonaspleyer.github.io/cellular_raza/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Simulation\n"
     ]
    }
   ],
   "source": [
    "output_path = run_simulation(simulation_settings)\n",
    "# output_path = \"out/autophagy/2023-10-31-00:19:21/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read results from json Files\n",
    "The results of the simulation are saved in json files.\n",
    "Due to the parallelized nature of the simulation, not all results are in one big json file but rather in multiple batches. We therefore need to combine these batches to obtain a complete set for a given iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def combine_batches(run_directory):\n",
    "    # Opens all batches in a given directory and stores\n",
    "    # them in one unified big list\n",
    "    combined_batch = []\n",
    "    for batch_file in os.listdir(run_directory):\n",
    "        f = open(run_directory / batch_file)\n",
    "        b = json.load(f)[\"data\"]\n",
    "        combined_batch.extend(b)\n",
    "    return combined_batch\n",
    "\n",
    "def get_cells_at_iterations(output_path):\n",
    "    # Uses the previously defined funtion [combine_batches]\n",
    "    # to read all stored cells at all iterations and stores\n",
    "    # them in a dictionary.\n",
    "    dir = Path(output_path) / \"cell_storage/json/\"\n",
    "    runs = [(x, dir / x) for x in os.listdir(dir)]\n",
    "    result = []\n",
    "    for (n_run, run_directory) in runs:\n",
    "        result.extend([{\"iteration\":int(n_run)} | c for c in combine_batches(run_directory)])\n",
    "    return result\n",
    "\n",
    "cells_at_iter = get_cells_at_iterations(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to inspect which entries our generated dataset has. Therefore, we normalize the dict, transforming it into a dataframe.\n",
    "Afterwards, we display all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration\n",
      "identifier\n",
      "element.id\n",
      "element.parent_id\n",
      "element.cell.mechanics.pos\n",
      "element.cell.mechanics.vel\n",
      "element.cell.mechanics.dampening_constant\n",
      "element.cell.mechanics.mass\n",
      "element.cell.mechanics.random_travel_velocity\n",
      "element.cell.mechanics.random_direction_travel\n",
      "element.cell.mechanics.random_update_time\n",
      "element.cell.interaction.species\n",
      "element.cell.interaction.cell_radius\n",
      "element.cell.interaction.potential_strength\n",
      "element.cell.interaction.interaction_range\n",
      "element.cell.interaction.clustering_strength\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.json_normalize(cells_at_iter)\n",
    "for col in df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Result\n",
    "We visualize the results in 3D.\n",
    "Therefore we use `pyvista` which internally uses `vtk` as a backend.\n",
    "Since all of our particles are represented as 3D-spheres, we also display them as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "def generate_spheres(df, iteration):\n",
    "    # Filter for only particles at the specified iteration\n",
    "    df_filtered = df[df[\"iteration\"]==iteration]\n",
    "\n",
    "    # Create a dataset for pyvista for plotting\n",
    "    pset = pv.PolyData(np.array([np.array(x) for x in df_filtered[\"element.cell.mechanics.pos\"]]))\n",
    "\n",
    "    # Extend dataset by species and diameter\n",
    "    pset.point_data[\"diameter\"] = 2.0*df_filtered[\"element.cell.interaction.cell_radius\"]\n",
    "    pset.point_data[\"species\"] = df_filtered[\"element.cell.interaction.species\"]\n",
    "\n",
    "    # Create spheres glyphs from dataset\n",
    "    sphere = pv.Sphere()\n",
    "    spheres = pset.glyph(geom=sphere, scale=\"diameter\", orient=False)\n",
    "\n",
    "    return spheres\n",
    "\n",
    "def save_snapshot(iteration):\n",
    "    spheres = generate_spheres(df, iteration)\n",
    "\n",
    "    spheres.plot(\n",
    "        off_screen=True,\n",
    "        screenshot=Path(output_path) / \"snapshot_{:08}.png\".format(iteration),\n",
    "        scalars=\"species\",\n",
    "        scalar_bar_args={\n",
    "            \"title\":\"Species\",\n",
    "        },\n",
    "        cpos=[\n",
    "            (\n",
    "                -1.5*simulation_settings.domain_size,\n",
    "                -1.5*simulation_settings.domain_size,\n",
    "                -1.5*simulation_settings.domain_size\n",
    "            ),(\n",
    "                50,\n",
    "                50,\n",
    "                50\n",
    "            ),(\n",
    "                0.0,\n",
    "                0.0,\n",
    "                0.0\n",
    "            )\n",
    "        ],\n",
    "        jupyter_backend='none',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save single snapshots or even use all processes of our device to save snapshots for every iteration.\n",
    "The 2nd approach will take up all resources by default. If you want to limit this, have a look at the [Pool object of the multiprocessing](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all snapshots\n",
    "with mp.Pool() as p:\n",
    "    p.map(save_snapshot, np.unique(df[\"iteration\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only save one snapshot\n",
    "save_snapshot(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
